{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed = 123, h1_size = 64, h2_size = 64, lr=5e-4):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_size, h1_size)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(h1_size, h2_size)\n",
    "        self.affine3 = nn.Linear(h2_size, action_size)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.affine2(x))\n",
    "        action_scores = self.affine3(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy(state_size, action_size)\n",
    "policy.load_state_dict(torch.load('checkpoint_re.pth'))\n",
    "policy.train()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward 1 : 7.00\n",
      "Episode Reward 2 : 3.00\n",
      "Episode Reward 3 : 8.00\n",
      "Episode Reward 4 : 5.00\n",
      "Episode Reward 5 : 5.00\n",
      "Episode Reward 6 : 3.00\n",
      "Episode Reward 7 : 0.00\n",
      "Episode Reward 8 : 3.00\n",
      "Episode Reward 9 : 0.00\n",
      "Episode Reward 10 : 3.00\n",
      "Episode Reward 11 : 2.00\n",
      "Episode Reward 12 : 5.00\n",
      "Episode Reward 13 : 5.00\n",
      "Episode Reward 14 : 1.00\n",
      "Episode Reward 15 : 8.00\n",
      "Episode Reward 16 : 3.00\n",
      "Episode Reward 17 : 4.00\n",
      "Episode Reward 18 : 2.00\n",
      "Episode Reward 19 : 5.00\n",
      "Episode Reward 20 : 3.00\n",
      "Episode Reward 21 : 5.00\n",
      "Episode Reward 22 : 9.00\n",
      "Episode Reward 23 : 6.00\n",
      "Episode Reward 24 : 9.00\n",
      "Episode Reward 25 : 2.00\n",
      "Episode Reward 26 : 5.00\n",
      "Episode Reward 27 : 2.00\n",
      "Episode Reward 28 : 8.00\n",
      "Episode Reward 29 : 1.00\n",
      "Episode Reward 30 : 3.00\n",
      "Episode Reward 31 : 4.00\n",
      "Episode Reward 32 : 7.00\n",
      "Episode Reward 33 : 8.00\n",
      "Episode Reward 34 : 6.00\n",
      "Episode Reward 35 : 4.00\n",
      "Episode Reward 36 : 2.00\n",
      "Episode Reward 37 : 6.00\n",
      "Episode Reward 38 : 0.00\n",
      "Episode Reward 39 : 2.00\n",
      "Episode Reward 40 : 2.00\n",
      "Episode Reward 41 : 5.00\n",
      "Episode Reward 42 : 8.00\n",
      "Episode Reward 43 : 5.00\n",
      "Episode Reward 44 : 3.00\n",
      "Episode Reward 45 : 8.00\n",
      "Episode Reward 46 : 6.00\n",
      "Episode Reward 47 : 5.00\n",
      "Episode Reward 48 : 9.00\n",
      "Episode Reward 49 : 3.00\n",
      "Episode Reward 50 : 2.00\n",
      "Episode Reward 51 : 3.00\n",
      "Episode Reward 52 : 3.00\n",
      "Episode Reward 53 : 8.00\n",
      "Episode Reward 54 : 5.00\n",
      "Episode Reward 55 : 7.00\n",
      "Episode Reward 56 : 3.00\n",
      "Episode Reward 57 : 10.00\n",
      "Episode Reward 58 : 8.00\n",
      "Episode Reward 59 : 5.00\n",
      "Episode Reward 60 : 5.00\n",
      "Episode Reward 61 : 9.00\n",
      "Episode Reward 62 : 3.00\n",
      "Episode Reward 63 : 4.00\n",
      "Episode Reward 64 : 6.00\n",
      "Episode Reward 65 : 6.00\n",
      "Episode Reward 66 : 2.00\n",
      "Episode Reward 67 : 7.00\n",
      "Episode Reward 68 : 6.00\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    scores_window = deque(maxlen=100)\n",
    "    max_score = -1\n",
    "    for i_episode in range(1, 1001):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 3000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]            \n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(ep_reward)\n",
    "        finish_episode()\n",
    "        print(\"Episode Reward {} : {:.2f}\".format(i_episode, ep_reward))\n",
    "        if i_episode % 100 == 0:\n",
    "            cur = np.mean(scores_window)\n",
    "            \n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, cur))\n",
    "            \n",
    "            torch.save(policy.state_dict(), 'checkpoint_' + str(i_episode) + str(cur) + '.pth')\n",
    "            \n",
    "            if cur >= 13:\n",
    "                torch.save(policy.state_dict(), 'checkpoint_final' + str(i_episode) + str(cur) + '.pth')\n",
    "            \n",
    "            if cur > max_score:\n",
    "                max_score = cur\n",
    "                torch.save(policy.state_dict(), 'checkpoint_re.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
